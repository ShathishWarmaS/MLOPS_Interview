name: MLOps Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: '3.9'
  DOCKER_REGISTRY: ghcr.io
  KUBECONFIG_PATH: ~/.kube/config

jobs:
  # Code Quality & Security
  quality-checks:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ env.PYTHON_VERSION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Lint code
        run: |
          echo "Running code linting..."
          flake8 training/ serving/ monitoring/ --max-line-length=100 --ignore=E203,W503
          black --check training/ serving/ monitoring/
          isort --check-only training/ serving/ monitoring/
      
      - name: Type checking
        run: |
          echo "Running type checking..."
          mypy training/ serving/ monitoring/ --ignore-missing-imports
      
      - name: Security scan
        run: |
          echo "Running security scans..."
          bandit -r training/ serving/ monitoring/ -f json -o bandit-report.json || true
          safety check --json --output safety-report.json || true
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
      
      - name: Code complexity analysis
        run: |
          echo "Analyzing code complexity..."
          radon cc training/ serving/ monitoring/ --min=B
          radon mi training/ serving/ monitoring/ --min=B

  # Data Validation
  data-validation:
    name: Data Quality Validation
    runs-on: ubuntu-latest
    needs: quality-checks
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Setup test environment
        run: |
          mkdir -p data/raw data/processed
          mkdir -p logs reports
      
      - name: Validate data schemas
        run: |
          echo "Validating data schemas..."
          python -c "
          import pandas as pd
          import numpy as np
          from training.pipelines.nodes import _generate_synthetic_data
          
          # Generate test data
          config = {'n_samples': 1000, 'n_features': 5}
          data = _generate_synthetic_data(config, {})
          
          # Basic schema validation
          assert 'target' in data.columns, 'Missing target column'
          assert len(data) == 1000, 'Incorrect number of samples'
          assert data.isnull().sum().sum() == 0, 'Unexpected null values'
          
          print('✅ Data schema validation passed')
          "
      
      - name: Check data quality
        run: |
          echo "Running data quality checks..."
          python -c "
          import sys
          sys.path.append('.')
          from training.pipelines.training_pipeline import PipelineRunner
          
          # Run data validation
          runner = PipelineRunner()
          runner.config['data_sources']['synthetic']['n_samples'] = 1000
          
          try:
              results = runner.run_data_pipeline()
              print('✅ Data quality validation passed')
              print(f'Generated data shape: {results[\"features\"].shape}')
          except Exception as e:
              print(f'❌ Data quality validation failed: {e}')
              sys.exit(1)
          "
      
      - name: Generate data report
        run: |
          echo "Generating data quality report..."
          python -c "
          import pandas as pd
          import json
          from datetime import datetime
          
          # Generate mock data report
          report = {
              'timestamp': datetime.now().isoformat(),
              'validation_status': 'passed',
              'data_sources': ['synthetic'],
              'total_records': 1000,
              'quality_checks': {
                  'completeness': 'passed',
                  'consistency': 'passed',
                  'validity': 'passed'
              }
          }
          
          with open('reports/data-quality-report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          print('✅ Data quality report generated')
          "
      
      - name: Upload data report
        uses: actions/upload-artifact@v3
        with:
          name: data-quality-report
          path: reports/data-quality-report.json

  # Model Training
  model-training:
    name: Model Training & Validation
    runs-on: ubuntu-latest
    needs: [quality-checks, data-validation]
    strategy:
      matrix:
        model: [baseline, advanced]
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Setup MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'sqlite:///mlflow.db' }}
        run: |
          echo "Setting up MLflow tracking..."
          export MLFLOW_TRACKING_USERNAME=${{ secrets.MLFLOW_USERNAME || 'admin' }}
          export MLFLOW_TRACKING_PASSWORD=${{ secrets.MLFLOW_PASSWORD || 'admin' }}
          
          # Start local MLflow server if no remote URI
          if [[ "$MLFLOW_TRACKING_URI" == sqlite* ]]; then
            mlflow server --host 127.0.0.1 --port 5000 --backend-store-uri $MLFLOW_TRACKING_URI &
            sleep 10
            export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
          fi
          
          echo "MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI" >> $GITHUB_ENV
      
      - name: Train model
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          echo "Training ${{ matrix.model }} model..."
          
          python -c "
          import sys
          sys.path.append('.')
          import mlflow
          from training.pipelines.training_pipeline import PipelineRunner
          
          # Set MLflow experiment
          mlflow.set_experiment('github_actions_${{ github.run_id }}')
          
          # Configure for fast training
          runner = PipelineRunner()
          runner.config['data_sources']['synthetic']['n_samples'] = 5000
          
          if '${{ matrix.model }}' == 'baseline':
              # Run only data pipeline and baseline training
              data_results = runner.run_data_pipeline()
              
              # Train baseline model only
              from training.pipelines.nodes import train_baseline_model
              baseline_result = train_baseline_model(
                  data_results['train_data'], 
                  runner.config['baseline_config']
              )
              print(f'✅ Baseline model trained with accuracy: {baseline_result[\"train_score\"]:.4f}')
          
          else:
              # Run full pipeline for advanced models
              results = runner.run_full_pipeline()
              model_info = results['registered_model']
              print(f'✅ Advanced models trained and registered: {model_info[\"model_name\"]}')
              print(f'Test metrics: {model_info[\"test_metrics\"]}')
          "
      
      - name: Validate model
        run: |
          echo "Validating ${{ matrix.model }} model..."
          
          python -c "
          import joblib
          import pandas as pd
          import numpy as np
          from pathlib import Path
          
          # Mock model validation
          print('Running model validation checks...')
          
          # Check 1: Model performance thresholds
          min_accuracy = 0.6
          mock_accuracy = 0.75  # Simulated accuracy
          assert mock_accuracy >= min_accuracy, f'Model accuracy {mock_accuracy} below threshold {min_accuracy}'
          
          # Check 2: Model size constraints
          max_model_size_mb = 100
          mock_size_mb = 5  # Simulated model size
          assert mock_size_mb <= max_model_size_mb, f'Model size {mock_size_mb}MB exceeds limit {max_model_size_mb}MB'
          
          # Check 3: Inference latency
          max_latency_ms = 1000
          mock_latency_ms = 50  # Simulated latency
          assert mock_latency_ms <= max_latency_ms, f'Model latency {mock_latency_ms}ms exceeds limit {max_latency_ms}ms'
          
          print('✅ Model validation passed')
          print(f'  Accuracy: {mock_accuracy}')
          print(f'  Size: {mock_size_mb}MB')
          print(f'  Latency: {mock_latency_ms}ms')
          "
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v3
        with:
          name: model-${{ matrix.model }}-${{ github.run_id }}
          path: |
            mlflow.db
            mlruns/

  # Model Selection
  model-selection:
    name: Model Selection & Registration
    runs-on: ubuntu-latest
    needs: model-training
    outputs:
      best-model: ${{ steps.model-selection.outputs.best_model }}
      model-version: ${{ steps.model-selection.outputs.model_version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download model artifacts
        uses: actions/download-artifact@v3
        with:
          pattern: model-*
          merge-multiple: true
      
      - name: Compare models
        id: model-comparison
        run: |
          echo "Comparing trained models..."
          
          python -c "
          import json
          import numpy as np
          
          # Mock model comparison results
          models = {
              'baseline': {
                  'accuracy': 0.75,
                  'f1_score': 0.73,
                  'roc_auc': 0.78,
                  'training_time': 60,
                  'model_size_mb': 2
              },
              'advanced': {
                  'accuracy': 0.82,
                  'f1_score': 0.81,
                  'roc_auc': 0.85,
                  'training_time': 300,
                  'model_size_mb': 15
              }
          }
          
          # Save comparison results
          with open('model_comparison.json', 'w') as f:
              json.dump(models, f, indent=2)
          
          print('✅ Model comparison completed')
          for model, metrics in models.items():
              print(f'{model}: F1={metrics[\"f1_score\"]:.3f}, AUC={metrics[\"roc_auc\"]:.3f}')
          "
      
      - name: Select best model
        id: model-selection
        run: |
          echo "Selecting best model based on criteria..."
          
          BEST_MODEL=$(python -c "
          import json
          
          with open('model_comparison.json', 'r') as f:
              models = json.load(f)
          
          # Selection criteria: F1 score with size constraint
          best_model = None
          best_score = 0
          
          for model, metrics in models.items():
              score = metrics['f1_score']
              size_penalty = 0.01 * metrics['model_size_mb']  # Penalize large models
              adjusted_score = score - size_penalty
              
              if adjusted_score > best_score:
                  best_score = adjusted_score
                  best_model = model
          
          print(best_model)
          ")
          
          MODEL_VERSION="v${{ github.run_number }}-$(echo ${{ github.sha }} | cut -c1-7)"
          
          echo "best_model=$BEST_MODEL" >> $GITHUB_OUTPUT
          echo "model_version=$MODEL_VERSION" >> $GITHUB_OUTPUT
          
          echo "✅ Selected best model: $BEST_MODEL (version: $MODEL_VERSION)"
      
      - name: Register model
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'sqlite:///mlflow.db' }}
        run: |
          echo "Registering model: ${{ steps.model-selection.outputs.best_model }}"
          
          python -c "
          import mlflow
          from datetime import datetime
          
          # Mock model registration
          model_name = 'mlops_pipeline_model'
          model_version = '${{ steps.model-selection.outputs.model_version }}'
          
          print(f'Registering model: {model_name} version {model_version}')
          print('✅ Model registration completed')
          "
      
      - name: Upload comparison report
        uses: actions/upload-artifact@v3
        with:
          name: model-comparison-report
          path: model_comparison.json

  # Container Build
  build-containers:
    name: Build & Push Containers
    runs-on: ubuntu-latest
    needs: model-selection
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/mlops-api
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
            type=raw,value=${{ needs.model-selection.outputs.model-version }}
      
      - name: Create Dockerfile
        run: |
          cat > Dockerfile << 'EOF'
          FROM python:3.9-slim
          
          WORKDIR /app
          
          # Install system dependencies
          RUN apt-get update && apt-get install -y \
              gcc \
              g++ \
              && rm -rf /var/lib/apt/lists/*
          
          # Copy requirements
          COPY requirements.txt .
          RUN pip install --no-cache-dir -r requirements.txt
          
          # Copy application code
          COPY serving/ ./serving/
          COPY training/ ./training/
          COPY monitoring/ ./monitoring/
          COPY config/ ./config/
          
          # Create non-root user
          RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
          USER appuser
          
          # Health check
          HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
            CMD curl -f http://localhost:8000/health || exit 1
          
          EXPOSE 8000
          
          CMD ["python", "-m", "serving.api.main"]
          EOF
      
      - name: Build and push
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}
            MODEL_VERSION=${{ needs.model-selection.outputs.model-version }}
      
      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/mlops-api:${{ github.sha }}
          format: spdx-json
          output-file: sbom.spdx.json
      
      - name: Upload SBOM
        uses: actions/upload-artifact@v3
        with:
          name: sbom
          path: sbom.spdx.json

  # Staging Deployment
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-containers
    environment: staging
    outputs:
      deployment-url: ${{ steps.deploy.outputs.url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Kubernetes
        run: |
          # Mock Kubernetes setup
          echo "Setting up Kubernetes access..."
          mkdir -p ~/.kube
          
          # In real scenario, decode KUBECONFIG_DATA secret
          echo "Kubernetes configuration ready"
      
      - name: Deploy to staging
        id: deploy
        run: |
          echo "Deploying to staging environment..."
          
          # Mock deployment
          IMAGE_TAG="${{ needs.build-containers.outputs.image-tag }}"
          DEPLOYMENT_URL="https://staging-${{ github.run_id }}.mlops.example.com"
          
          echo "Deployed image: $IMAGE_TAG"
          echo "url=$DEPLOYMENT_URL" >> $GITHUB_OUTPUT
          
          echo "✅ Staging deployment completed"
          echo "🌐 Staging URL: $DEPLOYMENT_URL"
      
      - name: Run smoke tests
        run: |
          echo "Running smoke tests against staging..."
          
          # Mock smoke tests
          python -c "
          import requests
          import time
          import json
          
          # Mock health check
          print('Testing health endpoint...')
          health_response = {
              'status': 'healthy',
              'version': '${{ needs.model-selection.outputs.model-version }}',
              'timestamp': time.time()
          }
          print(f'✅ Health check passed: {health_response}')
          
          # Mock prediction test
          print('Testing prediction endpoint...')
          prediction_response = {
              'prediction': 0.25,
              'confidence': 0.87,
              'model_version': '${{ needs.model-selection.outputs.model-version }}'
          }
          print(f'✅ Prediction test passed: {prediction_response}')
          
          print('✅ All smoke tests passed')
          "

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: deploy-staging
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install test dependencies
        run: |
          pip install pytest requests locust
      
      - name: Run integration tests
        run: |
          echo "Running integration tests..."
          
          python -c "
          import pytest
          import requests
          import time
          import json
          
          class TestMLOpsAPI:
              def test_health_endpoint(self):
                  # Mock health test
                  print('Testing /health endpoint...')
                  assert True
              
              def test_prediction_endpoint(self):
                  # Mock prediction test
                  print('Testing /predict endpoint...')
                  assert True
              
              def test_model_version(self):
                  # Mock version test
                  print('Testing model version consistency...')
                  assert True
              
              def test_performance_metrics(self):
                  # Mock performance test
                  print('Testing performance metrics...')
                  assert True
          
          # Run tests
          test_instance = TestMLOpsAPI()
          test_instance.test_health_endpoint()
          test_instance.test_prediction_endpoint()
          test_instance.test_model_version()
          test_instance.test_performance_metrics()
          
          print('✅ All integration tests passed')
          "
      
      - name: Run performance tests
        run: |
          echo "Running performance tests..."
          
          python -c "
          import time
          import statistics
          
          # Mock load testing
          print('Running load test simulation...')
          
          # Simulate latency measurements
          latencies = [45, 52, 38, 67, 43, 49, 55, 41, 48, 53]
          
          avg_latency = statistics.mean(latencies)
          p95_latency = sorted(latencies)[int(0.95 * len(latencies))]
          
          print(f'Average latency: {avg_latency:.1f}ms')
          print(f'95th percentile latency: {p95_latency:.1f}ms')
          
          # Performance assertions
          assert avg_latency < 100, f'Average latency {avg_latency}ms exceeds 100ms'
          assert p95_latency < 200, f'P95 latency {p95_latency}ms exceeds 200ms'
          
          print('✅ Performance tests passed')
          "

  # Production Deployment
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging, integration-tests, model-selection]
    environment: production
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Kubernetes
        run: |
          echo "Setting up production Kubernetes access..."
          # Mock production Kubernetes setup
          echo "Production Kubernetes configuration ready"
      
      - name: Deploy to production
        run: |
          echo "Deploying to production environment..."
          
          IMAGE_TAG="${{ needs.build-containers.outputs.image-tag }}"
          MODEL_VERSION="${{ needs.model-selection.outputs.model-version }}"
          
          echo "Deploying image: $IMAGE_TAG"
          echo "Model version: $MODEL_VERSION"
          
          # Mock production deployment
          echo "✅ Production deployment completed"
          echo "🌐 Production URL: https://api.mlops.example.com"
      
      - name: Verify deployment
        run: |
          echo "Verifying production deployment..."
          
          # Mock deployment verification
          python -c "
          import time
          
          print('Checking deployment status...')
          time.sleep(2)
          
          print('Verifying service health...')
          time.sleep(1)
          
          print('Checking model endpoint...')
          time.sleep(1)
          
          print('✅ Production deployment verified')
          "
      
      - name: Update model registry
        run: |
          echo "Updating model registry for production..."
          
          python -c "
          import json
          from datetime import datetime
          
          # Mock model registry update
          registry_update = {
              'model_name': 'mlops_pipeline_model',
              'version': '${{ needs.model-selection.outputs.model-version }}',
              'stage': 'Production',
              'deployed_at': datetime.now().isoformat(),
              'deployment_url': 'https://api.mlops.example.com'
          }
          
          print(f'Model registry updated: {json.dumps(registry_update, indent=2)}')
          print('✅ Model promoted to Production stage')
          "

  # Post-Deployment Monitoring
  post-deployment:
    name: Post-Deployment Setup
    runs-on: ubuntu-latest
    needs: deploy-production
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup monitoring
        run: |
          echo "Setting up production monitoring..."
          
          python -c "
          import json
          from datetime import datetime
          
          # Mock monitoring setup
          monitoring_config = {
              'environment': 'production',
              'model_version': '${{ needs.model-selection.outputs.model-version }}',
              'deployment_id': '${{ github.run_id }}',
              'monitoring_enabled': True,
              'alerts_configured': True,
              'dashboard_url': 'https://monitoring.mlops.example.com'
          }
          
          print(f'Monitoring configuration: {json.dumps(monitoring_config, indent=2)}')
          print('✅ Production monitoring configured')
          "
      
      - name: Configure alerts
        run: |
          echo "Configuring production alerts..."
          
          # Mock alert configuration
          python -c "
          alerts = [
              'Model accuracy below threshold',
              'API latency above SLA',
              'Error rate spike',
              'Resource utilization high'
          ]
          
          for alert in alerts:
              print(f'✅ Configured alert: {alert}')
          
          print('✅ All production alerts configured')
          "
      
      - name: Send deployment notification
        run: |
          echo "Sending deployment notification..."
          
          # Mock notification
          python -c "
          import json
          from datetime import datetime
          
          notification = {
              'type': 'deployment-success',
              'environment': 'production',
              'version': '${{ needs.model-selection.outputs.model-version }}',
              'timestamp': datetime.now().isoformat(),
              'commit': '${{ github.sha }}',
              'deployed_by': '${{ github.actor }}',
              'run_id': '${{ github.run_id }}'
          }
          
          print(f'Deployment notification: {json.dumps(notification, indent=2)}')
          print('✅ Deployment notification sent')
          "

  # Pipeline Summary
  pipeline-summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [quality-checks, data-validation, model-selection, build-containers, deploy-staging, integration-tests]
    if: always()
    steps:
      - name: Generate pipeline summary
        run: |
          echo "## 🚀 MLOps Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 📊 Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ needs.quality-checks.result == 'success' && '✅ Passed' || '❌ Failed' }} | Linting, type checking, security |" >> $GITHUB_STEP_SUMMARY
          echo "| Data Validation | ${{ needs.data-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} | Schema validation, quality checks |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Selection | ${{ needs.model-selection.result == 'success' && '✅ Passed' || '❌ Failed' }} | Best model: ${{ needs.model-selection.outputs.best-model }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Container Build | ${{ needs.build-containers.result == 'success' && '✅ Passed' || '❌ Failed' }} | Multi-arch container build |" >> $GITHUB_STEP_SUMMARY
          echo "| Staging Deploy | ${{ needs.deploy-staging.result == 'success' && '✅ Passed' || '❌ Failed' }} | Automated staging deployment |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} | End-to-end testing |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 🏷️ Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- **Model Version:** \`${{ needs.model-selection.outputs.model-version }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Container Image:** \`${{ needs.build-containers.outputs.image-tag }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Staging URL:** \`${{ needs.deploy-staging.outputs.deployment-url }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 📈 Next Steps" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "- ✅ Production deployment will proceed automatically" >> $GITHUB_STEP_SUMMARY
            echo "- 📊 Monitor production metrics and alerts" >> $GITHUB_STEP_SUMMARY
          else
            echo "- 🔄 Merge to main branch to trigger production deployment" >> $GITHUB_STEP_SUMMARY
            echo "- 🧪 Review staging environment for any issues" >> $GITHUB_STEP_SUMMARY
          fi